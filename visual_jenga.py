
import numpy as np
import torch
import matplotlib.pyplot as plt
import re
import cv2
from diffusers import AutoPipelineForInpainting
import torch
from PIL import Image, ImageOps, ImageFilter
import numpy as np
from utils import crop_to_mask, cosine_similarity_between_features
from dino_metric import get_dino_features
import copy
import argparse


from PIL import Image
from sam2.sam2_image_predictor import SAM2ImagePredictor
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    GenerationConfig,
    BitsAndBytesConfig
)

NUM_INPAINTING_TRIALS = 10
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

quant_config = BitsAndBytesConfig(load_in_4bit=True)
# Load SAM2 model.
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2.1-hiera-large")

# Load Molmo model.
processor = AutoProcessor.from_pretrained(
    'allenai/MolmoE-1B-0924', 
    trust_remote_code=True, 
    device_map='auto', 
    torch_dtype='auto'
)
model = AutoModelForCausalLM.from_pretrained(
    'allenai/MolmoE-1B-0924', 
    trust_remote_code=True, 
    offload_folder='offload', 
    quantization_config=quant_config, 
    torch_dtype='auto'
)

# Load the inpainting pipeline
pipe = AutoPipelineForInpainting.from_pretrained(
    "stable-diffusion-v1-5/stable-diffusion-v1-5", torch_dtype=torch.float16
).to(device)


def show_mask(mask):
    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    h, w = mask.shape[-2:]
    mask = mask.astype(np.uint8)
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    return mask_image


def show_masks(masks, scores):
    for i, (mask, score) in enumerate(zip(masks, scores)):
        if i == 0:  # Only show the highest scoring mask.
            mask_image = show_mask(mask)
    return mask_image


def get_coords(output_string, image):
    """
    Function to get x, y coordinates given Molmo model outputs.

    :param output_string: Output from the Molmo model.
    :param image: Image in PIL format.

    Returns:
        coordinates: Coordinates in format of [(x, y), (x, y)]
    """
    image = np.array(image)
    h, w = image.shape[:2]
    
    coordinates = None
    if 'points' in output_string:
        matches = re.findall(r'(x\d+)="([\d.]+)" (y\d+)="([\d.]+)"', output_string)
        coordinates = [(int(float(x_val)/100*w), int(float(y_val)/100*h)) for _, x_val, _, y_val in matches]
    else:
        match = re.search(r'x="([\d.]+)" y="([\d.]+)"', output_string)
        if match:
            coordinates = [(int(float(match.group(1))/100*w), int(float(match.group(2))/100*h))]
    
    return coordinates


def get_output(image, prompt='Describe this image.'):
    """
    Function to get output from Molmo model given an image and a prompt.

    :param image: PIL image.
    :param prompt: User prompt.

    Returns:
        generated_text: Output generated by the model.
    """
    inputs = processor.process(images=[image], text=prompt)
    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}
    
    output = model.generate_from_batch(
        inputs,
        GenerationConfig(max_new_tokens=200, stop_strings='<|endoftext|>'),
        tokenizer=processor.tokenizer
    )
    
    generated_tokens = output[0, inputs['input_ids'].size(1):]
    generated_text = processor.tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return generated_text


def get_masks(image, prompt):
    # Get coordinates from the model output.
    output = get_output(image, prompt)
    coords = get_coords(output, image)
    
    # Prepare input for SAM
    
    # assume each point is an independent object
    masks_pngs = []
    for point in coords:
        input_points = np.array([point])
        input_labels = np.ones(len(input_points), dtype=np.int32)
    
        # Convert image to numpy array if it's not already.
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        # Predict mask.
        predictor.set_image(image)
        with torch.no_grad():
            masks, scores, logits = predictor.predict(
                point_coords=input_points,
                point_labels=input_labels,
                multimask_output=False,
            )
        
        # Sort masks by score.
        sorted_ind = np.argsort(scores)[::-1]
        masks = masks[sorted_ind]
        scores = scores[sorted_ind]

        # TODO: don't hardcode this
        num_white = np.sum(masks[0])
        if num_white < 1000:
            continue

        mask_image = show_masks(masks, scores)
        masks_pngs.append(mask_image)

    return masks_pngs


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process a filename and a prompt.")
    parser.add_argument("filename", type=str, help="Path to the input file")
    parser.add_argument(
        "--prompt",
        type=str,
        help="Prompt text to process",
        default="point to objects in the image, both animals and furniture",
    )
    args = parser.parse_args()

    masks = get_masks(Image.open(args.filename).convert("RGB"), args.prompt)

    init_image = Image.open("meow.png").convert("RGB").resize((512, 512))
 
    f, ax = plt.subplots(len(masks), NUM_INPAINTING_TRIALS)
    
    for mask_idx, rgba_mask in enumerate(masks):
        rgba_mask = (rgba_mask > 0).astype(np.float32)
        rgba_mask = (rgba_mask * 255).astype(np.uint8)

        rgba_mask = Image.fromarray(rgba_mask, mode="RGBA")
        grayscale_mask = rgba_mask.convert("L")

        # Resize the image
        grayscale_mask = grayscale_mask.resize((512, 512), resample=Image.NEAREST)
        grayscale_mask = grayscale_mask.filter(ImageFilter.MaxFilter(size=11))

        prompt = "Full HD, 4K, high quality, high resolution, photorealistic"
        negative_prompt = "bad anatomy, bad proportions, blurry, cropped, deformed, disfigured, duplicate, error, extra limbs, gross proportions, jpeg artifacts, long neck, low quality, lowres, malformed, morbid, mutated, mutilated, out of frame, ugly, worst quality"

        
        for trial_num in range(NUM_INPAINTING_TRIALS):
            # Perform inpainting
            result = pipe(
                prompt=prompt,
                image=init_image,
                mask_image=grayscale_mask,
                num_inference_steps=400,
                negative_prompt=negative_prompt,
            ).images[0]
            
            ax[mask_idx][trial_num].imshow(result)
            ax[mask_idx][trial_num].axis('off')

            crop1 = crop_to_mask(result, grayscale_mask)
            crop2 = crop_to_mask(init_image, grayscale_mask)

            crop1_dino = get_dino_features(crop1)
            crop2_dino = get_dino_features(crop2)

            cos_sim = cosine_similarity_between_features(crop1_dino, crop2_dino)
            ax[mask_idx][trial_num].set_title(f"{round(cos_sim,3)}", fontsize=10)
    f.savefig("masked.png")
