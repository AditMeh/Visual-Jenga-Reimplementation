import numpy as np
import torch
import matplotlib.pyplot as plt
import re
from pathlib import Path
import cv2
from diffusers import AutoPipelineForInpainting
import torch
from PIL import Image, ImageOps, ImageFilter
import numpy as np
from utils import (
    crop_to_mask,
    cosine_similarity_between_features,
    preprocess_image,
    preprocess_mask,
)
from dino_metric import get_dino_features
import copy
import argparse
from diffusers import StableDiffusionInpaintPipeline, DiffusionPipeline
from diffusers import DDIMScheduler, DiffusionPipeline

from PIL import Image
from sam2.sam2_image_predictor import SAM2ImagePredictor
from transformers import (
    AutoModelForCausalLM,
    AutoProcessor,
    GenerationConfig,
    BitsAndBytesConfig,
)

NUM_INPAINTING_TRIALS = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load SAM2 model
predictor = SAM2ImagePredictor.from_pretrained("facebook/sam2.1-hiera-large")

# Load Molmo model
processor = AutoProcessor.from_pretrained(
    "allenai/Molmo-7B-D-0924",
    trust_remote_code=True,
    device_map="auto",
    # torch_dtype=torch.float16,
)

model = AutoModelForCausalLM.from_pretrained(
    "allenai/Molmo-7B-D-0924",
    trust_remote_code=True,
    offload_folder="offload",
    # torch_dtype=torch.float16,
).to("cuda")


# Load inpainting model
pipe = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-2-inpainting",
    torch_dtype=torch.float16,
    variant="fp16",
).to("cuda")

# Load eraser pipeline
scheduler = DDIMScheduler(
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear",
    clip_sample=False,
    set_alpha_to_one=False,
)
eraser_pipeline = DiffusionPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    custom_pipeline="pipeline_stable_diffusion_xl_attentive_eraser",
    scheduler=scheduler,
    variant="fp16",
    use_safetensors=True,
    torch_dtype=torch.float16,
).to(device)


def show_mask(mask):
    color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    h, w = mask.shape[-2:]
    mask = mask.astype(np.uint8)
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    return mask_image


def show_masks(masks, scores):
    for i, (mask, score) in enumerate(zip(masks, scores)):
        if i == 0:  # Only show the highest scoring mask.
            mask_image = show_mask(mask)
    return mask_image


def get_coords(output_string, image):
    """
    Function to get x, y coordinates given Molmo model outputs.

    :param output_string: Output from the Molmo model.
    :param image: Image in PIL format.

    Returns:
        coordinates: Coordinates in format of [(x, y), (x, y)]
    """
    image = np.array(image)
    h, w = image.shape[:2]

    coordinates = None
    if "points" in output_string:
        matches = re.findall(r'(x\d+)="([\d.]+)" (y\d+)="([\d.]+)"', output_string)
        coordinates = [
            (int(float(x_val) / 100 * w), int(float(y_val) / 100 * h))
            for _, x_val, _, y_val in matches
        ]
    else:
        match = re.search(r'x="([\d.]+)" y="([\d.]+)"', output_string)
        if match:
            coordinates = [
                (
                    int(float(match.group(1)) / 100 * w),
                    int(float(match.group(2)) / 100 * h),
                )
            ]

    return coordinates


def get_output(image, prompt="Describe this image."):
    """
    Function to get output from Molmo model given an image and a prompt.

    :param image: PIL image.
    :param prompt: User prompt.

    Returns:
        generated_text: Output generated by the model.
    """
    inputs = processor.process(images=[image], text=prompt)
    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}

    output = model.generate_from_batch(
        inputs,
        GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),
        tokenizer=processor.tokenizer,
    )

    generated_tokens = output[0, inputs["input_ids"].size(1) :]
    generated_text = processor.tokenizer.decode(
        generated_tokens, skip_special_tokens=True
    )

    return generated_text


def upsample_mask_nearest(rgba_mask):
    rgba_mask = (rgba_mask > 0).astype(np.float32)
    rgba_mask = (rgba_mask * 255).astype(np.uint8)

    rgba_mask = Image.fromarray(rgba_mask, mode="RGBA")
    grayscale_mask = rgba_mask.convert("L")

    # Resize the image
    grayscale_mask = grayscale_mask.resize((512, 512), resample=Image.NEAREST)
    grayscale_mask = grayscale_mask.filter(ImageFilter.MaxFilter(size=13))
    return grayscale_mask


def get_masks(image, prompt):
    # Get coordinates from the model output.
    output = get_output(image, prompt)
    print(output)
    coords = get_coords(output, image)
    masks_pngs = []

    if coords == None:
        return None

    # assume each point is an independent object
    for point in coords:
        input_points = np.array([point])
        input_labels = np.ones(len(input_points), dtype=np.int32)

        # Convert image to numpy array if it's not already.
        if isinstance(image, Image.Image):
            image = np.array(image)

        # Predict mask.
        predictor.set_image(image)
        with torch.no_grad():
            masks, scores, logits = predictor.predict(
                point_coords=input_points,
                point_labels=input_labels,
                multimask_output=False,
            )

        # Sort masks by score.
        sorted_ind = np.argsort(scores)[::-1]
        masks = masks[sorted_ind]
        scores = scores[sorted_ind]

        # TODO: don't hardcode this
        num_white = np.sum(masks[0])
        if num_white < 1000:
            print("skipped mask")
            continue

        mask_image = show_masks(masks, scores)
        masks_pngs.append(mask_image)

    return masks_pngs


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process a filename and a prompt.")
    parser.add_argument("filename", type=str, help="Path to the input file")
    parser.add_argument(
        "--prompt",
        type=str,
        help="List of prompt texts to process",
        default="point to all books in the image",
    )
    args = parser.parse_args()

    masks = get_masks(Image.open(args.filename).convert("RGB"), args.prompt)
    init_image = Image.open(args.filename).convert("RGB").resize((512, 512))

    counter = 0
    while masks is not None:
        best_mask_idx, max_performance = -1, np.inf
        for mask_idx, rgba_mask in enumerate(masks):

            grayscale_mask = upsample_mask_nearest(rgba_mask)

            prompt = "Full HD, 4K, high quality, high resolution, photorealistic"
            negative_prompt = "bad anatomy, bad proportions, blurry, cropped, deformed, disfigured, duplicate, error, extra limbs, gross proportions, jpeg artifacts, long neck, low quality, lowres, malformed, morbid, mutated, mutilated, out of frame, ugly, worst quality"

            cos_sim_accum = 0
            for trial_num in range(NUM_INPAINTING_TRIALS):
                # Perform inpainting
                result = pipe(
                    prompt=prompt,
                    image=init_image,
                    mask_image=grayscale_mask,
                    num_inference_steps=100,
                    negative_prompt=negative_prompt,
                ).images[0]

                crop1 = crop_to_mask(result, grayscale_mask)
                crop2 = crop_to_mask(init_image, grayscale_mask)

                crop1_dino = get_dino_features(crop1)
                crop2_dino = get_dino_features(crop2)

                cos_sim = cosine_similarity_between_features(crop1_dino, crop2_dino)
                cos_sim_accum += cos_sim

            if cos_sim_accum / NUM_INPAINTING_TRIALS < max_performance:
                max_performance = cos_sim_accum / NUM_INPAINTING_TRIALS
                best_mask_idx = mask_idx

        erased_image = eraser_pipeline(
            prompt="",
            image=preprocess_image(init_image, device),
            mask_image=preprocess_mask(
                upsample_mask_nearest(masks[best_mask_idx]), device
            ),
            height=1024,
            width=1024,
            AAS=True,  # enable AAS
            strength=0.8,  # inpainting strength
            rm_guidance_scale=9,  # removal guidance scale
            ss_steps=9,  # similarity suppression steps
            ss_scale=0.3,  # similarity suppression scale
            AAS_start_step=0,  # AAS start step
            AAS_start_layer=34,  # AAS start layer
            AAS_end_layer=70,  # AAS end layer
            num_inference_steps=150,  # number of inference steps
            generator=torch.Generator(device=device).manual_seed(123),
            guidance_scale=1,
        ).images[0]

        counter += 1

        init_image = erased_image.resize((512, 512)).convert("RGB")
        init_image.save(f"{counter}.png")

        masks = get_masks(init_image, args.prompt)
